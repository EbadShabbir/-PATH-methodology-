{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.utils.prune as prune\nimport torch.optim as optim\nimport torch.quantization\nfrom torch.cuda.amp import autocast, GradScaler\nfrom collections import Counter\nimport numpy as np\nfrom scipy.stats import entropy\n\n# Define a simple model for demonstration\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n# Initialize the model, optimizer, and scaler\nmodel = SimpleModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscaler = GradScaler()  # For AMP training stability\n\n# Example input and target\ninputs = torch.randn(128, 784)  # 128 samples, 784 features each\ntargets = torch.randint(0, 10, (128,))  # 128 targets for classification\n\n# --- Step 1: Pruning using Taylor Method ---\ndef taylor_pruning(model, inputs, targets, pruning_amount=0.2):\n    model.eval()\n    loss_fn = nn.CrossEntropyLoss()\n    outputs = model(inputs)\n    loss = loss_fn(outputs, targets)\n    loss.backward()  # Compute gradients for Taylor-based importance\n\n    # Prune weights based on gradient magnitude (approximation of Taylor method)\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            prune.l1_unstructured(module, name=\"weight\", amount=pruning_amount)\n    print(\"Pruning applied!\")\n\ntaylor_pruning(model, inputs, targets, pruning_amount=0.2)\n\n# --- Step 2: Quantization-Aware Training (QAT) ---\ndef apply_quantization(model):\n    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n    torch.quantization.prepare_qat(model, inplace=True)\n    print(\"Quantization-aware training setup complete.\")\n    return model\n\nmodel = apply_quantization(model)\n\n# --- Step 3: Huffman Coding ---\ndef huffman_encoding(tensor):\n    # Flatten weights and compute frequencies\n    flat_tensor = tensor.detach().cpu().numpy().flatten()\n    frequencies = Counter(flat_tensor)\n    prob_dist = np.array(list(frequencies.values())) / len(flat_tensor)\n    print(f\"Compression Entropy: {entropy(prob_dist, base=2)} bits\")\n\n    # Placeholder for Huffman encoding logic (to be implemented for actual compression)\n    return frequencies\n\n# Example: Apply Huffman encoding to quantized weights\ndef apply_huffman_to_model(model):\n    for name, param in model.named_parameters():\n        if \"weight\" in name:\n            print(f\"Applying Huffman coding to {name}...\")\n            huffman_encoding(param)\n\n# Huffman coding applied post-QAT conversion\napply_huffman_to_model(model)\n\n# --- Step 4: Training with Automatic Mixed Precision (AMP) ---\ndef train_step_amp(model, optimizer, inputs, targets, scaler):\n    model.train()\n    optimizer.zero_grad()\n\n    with autocast():  # Use mixed precision\n        outputs = model(inputs)\n        loss = nn.CrossEntropyLoss()(outputs, targets)\n\n    # Scale gradients, backward pass, and optimizer step\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    return loss.item()\n\n# Train the model with QAT + AMP + Huffman-ready weights\nbatch_size = 32\nfor epoch in range(3):  # Example 3 epochs\n    for i in range(0, inputs.size(0), batch_size):\n        batch_inputs = inputs[i:i+batch_size]\n        batch_targets = targets[i:i+batch_size]\n        loss = train_step_amp(model, optimizer, batch_inputs, batch_targets, scaler)\n        print(f\"Epoch {epoch}, Batch Loss: {loss}\")\n\n# Convert QAT model to fully quantized\nmodel = torch.quantization.convert(model.eval(), inplace=True)\nprint(\"Model quantized and ready for deployment.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:34:49.316757Z","iopub.execute_input":"2024-11-22T15:34:49.317146Z","iopub.status.idle":"2024-11-22T15:34:56.614582Z","shell.execute_reply.started":"2024-11-22T15:34:49.317111Z","shell.execute_reply":"2024-11-22T15:34:56.612834Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2317472907.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # For AMP training stability\n/opt/conda/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Pruning applied!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantization-aware training setup complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mapply_quantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# --- Step 3: Huffman Coding ---\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhuffman_encoding\u001b[39m(tensor):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Flatten weights and compute frequencies\u001b[39;00m\n","Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mapply_quantization\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_quantization\u001b[39m(model):\n\u001b[1;32m     51\u001b[0m     model\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mget_default_qat_qconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfbgemm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_qat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantization-aware training setup complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:495\u001b[0m, in \u001b[0;36mprepare_qat\u001b[0;34m(model, mapping, inplace)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03mPrepares a copy of the model for quantization calibration or\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mquantization-aware training and converts it to quantized version.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m             is mutated\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize.prepare_qat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_qat only works on models in training mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     mapping \u001b[38;5;241m=\u001b[39m get_default_qat_module_mappings()\n","\u001b[0;31mAssertionError\u001b[0m: prepare_qat only works on models in training mode"],"ename":"AssertionError","evalue":"prepare_qat only works on models in training mode","output_type":"error"}],"execution_count":1}]}